{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231512a0-7cb0-46b4-a8c5-b665b6297c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===x torch.Size([32, 30, 100, 20])\n",
      "===x_res torch.Size([32, 30, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out== torch.Size([32, 30, 100, 20])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 30, 100, 20])\n",
      "1360\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "sys.path.insert(0, '')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def activation_factory(name, inplace=True):\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU(inplace=inplace)\n",
    "    elif name == 'leakyrelu':\n",
    "        return nn.LeakyReLU(0.2, inplace=inplace)\n",
    "    elif name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif name == 'linear' or name is None:\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError('Not supported activation:', name)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        pad = (kernel_size + (kernel_size-1) * (dilation-1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                activation_factory(activation),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation),\n",
    "            )\n",
    "            for dilation in dilations\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            activation_factory(activation),\n",
    "            nn.MaxPool2d(kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride,1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = activation_factory(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        print('===x',np.shape(x))\n",
    "        res = self.residual(x)\n",
    "        print('===x_res',np.shape(res))\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            print('===out',np.shape(out))\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv(30, 30)\n",
    "    x = torch.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn.forward(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "        #print(f'{name}: {param.numel()}')\n",
    "    print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fc21a3-9d92-4f91-9564-5f866ad92e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:23.661835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.671664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.672369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.673462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 13:12:23.673908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.674584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.675278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.092150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3347 MB memory:  -> device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:24.564350: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n",
      "2023-06-14 13:12:24.767888: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 30, 100, 20)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#keras.layers.Conv2D(kernel_size=(1, 6), strides=(1, 6), padding='same', input_shape=input_shape[1:])\n",
    "\n",
    "\n",
    "\n",
    "class TemporalConv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "\n",
    "        self.conv = layers.Conv2D(\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding='same',  # Set padding to 'valid'\n",
    "            strides=(stride, 1),\n",
    "            dilation_rate=(dilation, 1),\n",
    "            data_format='channels_first')\n",
    "\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = layers.ZeroPadding2D(padding=((self.pad, self.pad), (0, 0)))(x)  # Apply padding\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv2(keras.Model):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super(MultiScale_TemporalConv2, self).__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = []\n",
    "        for dilation in dilations:\n",
    "            self.branches.append(\n",
    "                keras.Sequential([\n",
    "                    layers.Conv2D(\n",
    "                        branch_channels,\n",
    "                        kernel_size=1,\n",
    "                        padding='valid',\n",
    "                        data_format='channels_first'),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Activation(activation),\n",
    "                    TemporalConv(\n",
    "                        branch_channels,\n",
    "                        branch_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        dilation=dilation)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid',data_format='channels_first'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation(activation),\n",
    "                layers.MaxPool2D(pool_size=(3,1), strides=(stride,1), padding='same'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid', strides=(stride,1),data_format='channels_first'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = layers.Activation(activation)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        #print(\"TCN_x\",np.shape(x) )\n",
    "        res = self.residual(x)\n",
    "        #print(\"TCN_res\",np.shape(res) )\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            #print('===out',np.shape(out))\n",
    "        #print(\"TCN_out\",np.shape(out) )\n",
    "        out = tf.concat(branch_outs, axis=1)\n",
    "        #print(\"TCN_out+concat\",np.shape(out) )\n",
    "        #print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv2(30, 30)\n",
    "    input_shape = (32, 30, 100, 20)\n",
    "    print( input_shape[1:] )\n",
    "    x = np.random.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "    #    print(f'{name}: {param.numel()}')\n",
    "    #print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a56327-dc41-44d5-8414-050919c55852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a81ff-ec1e-4321-9a56-4c5aed1da730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
