{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231512a0-7cb0-46b4-a8c5-b665b6297c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===x torch.Size([32, 30, 100, 20])\n",
      "===x_res torch.Size([32, 30, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out== torch.Size([32, 30, 100, 20])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 30, 100, 20])\n",
      "1360\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "sys.path.insert(0, '')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def activation_factory(name, inplace=True):\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU(inplace=inplace)\n",
    "    elif name == 'leakyrelu':\n",
    "        return nn.LeakyReLU(0.2, inplace=inplace)\n",
    "    elif name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif name == 'linear' or name is None:\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError('Not supported activation:', name)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        pad = (kernel_size + (kernel_size-1) * (dilation-1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                activation_factory(activation),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation),\n",
    "            )\n",
    "            for dilation in dilations\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            activation_factory(activation),\n",
    "            nn.MaxPool2d(kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride,1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = activation_factory(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        print('===x',np.shape(x))\n",
    "        res = self.residual(x)\n",
    "        print('===x_res',np.shape(res))\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            print('===out',np.shape(out))\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv(30, 30)\n",
    "    x = torch.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn.forward(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "        #print(f'{name}: {param.numel()}')\n",
    "    print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fc21a3-9d92-4f91-9564-5f866ad92e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:23.661835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.671664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.672369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.673462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 13:12:23.673908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.674584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.675278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.092150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3347 MB memory:  -> device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:24.564350: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n",
      "2023-06-14 13:12:24.767888: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 30, 100, 20)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#keras.layers.Conv2D(kernel_size=(1, 6), strides=(1, 6), padding='same', input_shape=input_shape[1:])\n",
    "\n",
    "\n",
    "\n",
    "class TemporalConv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "\n",
    "        self.conv = layers.Conv2D(\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding='same',  # Set padding to 'valid'\n",
    "            strides=(stride, 1),\n",
    "            dilation_rate=(dilation, 1),\n",
    "            data_format='channels_first')\n",
    "\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = layers.ZeroPadding2D(padding=((self.pad, self.pad), (0, 0)))(x)  # Apply padding\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv2(keras.Model):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super(MultiScale_TemporalConv2, self).__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = []\n",
    "        for dilation in dilations:\n",
    "            self.branches.append(\n",
    "                keras.Sequential([\n",
    "                    layers.Conv2D(\n",
    "                        branch_channels,\n",
    "                        kernel_size=1,\n",
    "                        padding='valid',\n",
    "                        data_format='channels_first'),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Activation(activation),\n",
    "                    TemporalConv(\n",
    "                        branch_channels,\n",
    "                        branch_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        dilation=dilation)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid',data_format='channels_first'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation(activation),\n",
    "                layers.MaxPool2D(pool_size=(3,1), strides=(stride,1), padding='same'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid', strides=(stride,1),data_format='channels_first'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = layers.Activation(activation)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        #print(\"TCN_x\",np.shape(x) )\n",
    "        res = self.residual(x)\n",
    "        #print(\"TCN_res\",np.shape(res) )\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            #print('===out',np.shape(out))\n",
    "        #print(\"TCN_out\",np.shape(out) )\n",
    "        out = tf.concat(branch_outs, axis=1)\n",
    "        #print(\"TCN_out+concat\",np.shape(out) )\n",
    "        #print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv2(30, 30)\n",
    "    input_shape = (32, 30, 100, 20)\n",
    "    print( input_shape[1:] )\n",
    "    x = np.random.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "    #    print(f'{name}: {param.numel()}')\n",
    "    #print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11aa1d5c-653d-429c-8592-26e91690e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_adjacency_matrix_tensor( inputs):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    kp = 13\n",
    "    if kp==13:\n",
    "        edges = [ (0,1), (1,2), (1,4), (2,3), (0,4), (4,5), (5,6), (1,7), (7,8), (8,9), (7,10), (4,10), (10,11), (11,12) ]\n",
    "    else:\n",
    "        edges = [ (0,1), (1,2), (2,3), (3,4), (1,5), (5,6), (6,7), (1,8), (8,9), (9,10), (10,11), (8,12), (12,13), (13,14) ]\n",
    "    # Initialize an empty matrix\n",
    "    adj_matrix = np.zeros((kp, kp))\n",
    "    # Iterate over the edges and set the corresponding entries in the matrix to 1\n",
    "    for edge in edges:\n",
    "        adj_matrix[edge[0], edge[1]] = 1\n",
    "        adj_matrix[edge[1], edge[0]] = 1\n",
    "\n",
    "    adj_matrix_tensor = tf.convert_to_tensor(adj_matrix, dtype=tf.float32)\n",
    "    adj_matrix_tensor = tf.expand_dims(adj_matrix_tensor, axis=0)\n",
    "    adj_matrix_tensor = tf.expand_dims(adj_matrix_tensor, axis=0)\n",
    "    adj_matrix_tensor = tf.tile(adj_matrix_tensor, [batch_size, 30 , 1, 1])\n",
    "    constant_tensor = tf.ones(shape=(batch_size, 30 , 13, 13))\n",
    "    constant_tensor = constant_tensor * adj_matrix_tensor\n",
    "    return constant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a171cdc3-62bd-4f6f-8fa5-1b83affbecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 30, 52)\n",
      "<class 'keras.engine.keras_tensor.KerasTensor'>\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=( 30 , 13*4))\n",
    "print(np.shape(inputs))\n",
    "A_binary = create_adjacency_matrix_tensor(inputs)\n",
    "print(type(A_binary[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a5a81ff-ec1e-4321-9a56-4c5aed1da730",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMS_GCN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiScale_GraphConv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ndarray \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_binary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m msgcn \u001b[38;5;241m=\u001b[39m MultiScale_GraphConv(num_scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, A_binary\u001b[38;5;241m=\u001b[39mndarray)\n\u001b[1;32m      8\u001b[0m msgcn(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal((\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m13\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/action/lib/python3.8/site-packages/keras/backend.py:3927\u001b[0m, in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3925\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   3926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m-> 3927\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\n\u001b[1;32m   3928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_in_graph_mode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3929\u001b[0m   \u001b[38;5;66;03m# This is a variable which was created in an eager context, but is being\u001b[39;00m\n\u001b[1;32m   3930\u001b[0m   \u001b[38;5;66;03m# evaluated from a Graph.\u001b[39;00m\n\u001b[1;32m   3931\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39meager_context\u001b[38;5;241m.\u001b[39meager_mode():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "from utils.MS_GCN import MultiScale_GraphConv\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "ndarray = K.get_value(A_binary[0][0])\n",
    "\n",
    "\n",
    "msgcn = MultiScale_GraphConv(num_scales=15, in_channels=4, out_channels=64, A_binary=ndarray)\n",
    "msgcn(tf.random.normal((16, 4, 30, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a7e1b-874b-4570-b900-4cdad7606906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9bc79-0c60-4b7e-9d79-d382512c3718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a57746-c47c-450c-88b2-26dcb9cc8d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
