{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231512a0-7cb0-46b4-a8c5-b665b6297c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===x torch.Size([32, 30, 100, 20])\n",
      "===x_res torch.Size([32, 30, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out torch.Size([32, 5, 100, 20])\n",
      "===out== torch.Size([32, 30, 100, 20])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 30, 100, 20])\n",
      "1360\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "sys.path.insert(0, '')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def activation_factory(name, inplace=True):\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU(inplace=inplace)\n",
    "    elif name == 'leakyrelu':\n",
    "        return nn.LeakyReLU(0.2, inplace=inplace)\n",
    "    elif name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif name == 'linear' or name is None:\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError('Not supported activation:', name)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        pad = (kernel_size + (kernel_size-1) * (dilation-1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                activation_factory(activation),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation),\n",
    "            )\n",
    "            for dilation in dilations\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            activation_factory(activation),\n",
    "            nn.MaxPool2d(kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride,1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = activation_factory(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        print('===x',np.shape(x))\n",
    "        res = self.residual(x)\n",
    "        print('===x_res',np.shape(res))\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            print('===out',np.shape(out))\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv(30, 30)\n",
    "    x = torch.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn.forward(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "        #print(f'{name}: {param.numel()}')\n",
    "    print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fc21a3-9d92-4f91-9564-5f866ad92e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:23.661835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.671664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.672369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.673462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 13:12:23.673908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.674584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:23.675278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.091868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 13:12:24.092150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3347 MB memory:  -> device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:12:24.564350: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n",
      "2023-06-14 13:12:24.767888: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 30, 100, 20)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#keras.layers.Conv2D(kernel_size=(1, 6), strides=(1, 6), padding='same', input_shape=input_shape[1:])\n",
    "\n",
    "\n",
    "\n",
    "class TemporalConv(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "\n",
    "        self.conv = layers.Conv2D(\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding='same',  # Set padding to 'valid'\n",
    "            strides=(stride, 1),\n",
    "            dilation_rate=(dilation, 1),\n",
    "            data_format='channels_first')\n",
    "\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = layers.ZeroPadding2D(padding=((self.pad, self.pad), (0, 0)))(x)  # Apply padding\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv2(keras.Model):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super(MultiScale_TemporalConv2, self).__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = []\n",
    "        for dilation in dilations:\n",
    "            self.branches.append(\n",
    "                keras.Sequential([\n",
    "                    layers.Conv2D(\n",
    "                        branch_channels,\n",
    "                        kernel_size=1,\n",
    "                        padding='valid',\n",
    "                        data_format='channels_first'),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Activation(activation),\n",
    "                    TemporalConv(\n",
    "                        branch_channels,\n",
    "                        branch_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        dilation=dilation)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid',data_format='channels_first'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation(activation),\n",
    "                layers.MaxPool2D(pool_size=(3,1), strides=(stride,1), padding='same'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        self.branches.append(\n",
    "            keras.Sequential([\n",
    "                layers.Conv2D(branch_channels, kernel_size=1, padding='valid', strides=(stride,1),data_format='channels_first'),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        self.act = layers.Activation(activation)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        #print(\"TCN_x\",np.shape(x) )\n",
    "        res = self.residual(x)\n",
    "        #print(\"TCN_res\",np.shape(res) )\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "            #print('===out',np.shape(out))\n",
    "        #print(\"TCN_out\",np.shape(out) )\n",
    "        out = tf.concat(branch_outs, axis=1)\n",
    "        #print(\"TCN_out+concat\",np.shape(out) )\n",
    "        #print('===out==',np.shape(out))\n",
    "        out += res\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mstcn = MultiScale_TemporalConv2(30, 30)\n",
    "    input_shape = (32, 30, 100, 20)\n",
    "    print( input_shape[1:] )\n",
    "    x = np.random.randn(32, 30, 100, 20)\n",
    "    x2 = mstcn(x)\n",
    "    print( type(x2) )\n",
    "    print( np.shape(x2) )\n",
    "    #for name, param in mstcn.named_parameters():\n",
    "    #    print(f'{name}: {param.numel()}')\n",
    "    #print(sum(p.numel() for p in mstcn.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfb4d8f-96df-4015-8dec-086ee2ca3547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30c270b-08aa-466f-bdce-30369dba9868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc354dd4-6c3b-4f26-a0b1-a693e2adb10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990711f9-c8b7-4fcd-ac74-4d6030137b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6484a8d-6bab-4a7f-aacc-63ae9a9d3612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174a3027-cb3f-4480-b680-1cc7ceed706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_adjacency_matrix_tensor(inputs):\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    kp = 13\n",
    "    if kp==13:\n",
    "        edges = [ (0,1), (1,2), (1,4), (2,3), (0,4), (4,5), (5,6), (1,7), (7,8), (8,9), (7,10), (4,10), (10,11), (11,12) ]\n",
    "    else:\n",
    "        edges = [ (0,1), (1,2), (2,3), (3,4), (1,5), (5,6), (6,7), (1,8), (8,9), (9,10), (10,11), (8,12), (12,13), (13,14) ]\n",
    "    # Initialize an empty matrix\n",
    "    adj_matrix = np.zeros((kp, kp))\n",
    "    # Iterate over the edges and set the corresponding entries in the matrix to 1\n",
    "    for edge in edges:\n",
    "        adj_matrix[edge[0], edge[1]] = 1\n",
    "        adj_matrix[edge[1], edge[0]] = 1\n",
    "\n",
    "    adj_matrix_tensor = tf.convert_to_tensor(adj_matrix, dtype=tf.float32)\n",
    "    adj_matrix_tensor = tf.expand_dims(adj_matrix_tensor, axis=0)\n",
    "    adj_matrix_tensor = tf.expand_dims(adj_matrix_tensor, axis=0)\n",
    "    adj_matrix_tensor = tf.tile(adj_matrix_tensor, [batch_size, 30 , 1, 1])\n",
    "    constant_tensor = tf.ones(shape=(batch_size, 30 , 13, 13))\n",
    "    constant_tensor = constant_tensor * adj_matrix_tensor\n",
    "    return constant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2c8cf2-fb15-49cc-97e3-e9ba39f05b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import utils.Graphtools as tools\n",
    "\n",
    "num_node = 13\n",
    "self_link = [(i, i) for i in range(num_node)]\n",
    "inward_ori_index = [ (0,1), (1,2), (1,4), (2,3), (0,4), (4,5), (5,6), (1,7), (7,8), (8,9), (7,10), (4,10), (10,11), (11,12) ]\n",
    "inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "outward = [(j, i) for (i, j) in inward]\n",
    "neighbor = inward + outward\n",
    "\n",
    "\n",
    "class AdjMatrixGraph:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.edges = neighbor\n",
    "        self.num_nodes = num_node\n",
    "        self.self_loops = [(i, i) for i in range(self.num_nodes)]\n",
    "        self.A_binary = tools.get_adjacency_matrix(self.edges, self.num_nodes)\n",
    "        self.A_binary_with_I = tools.get_adjacency_matrix(self.edges + self.self_loops, self.num_nodes)\n",
    "        self.A = tools.normalize_adjacency_matrix(self.A_binary)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b0de7-d515-4002-a8dd-f07324a265d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6894584c-8e21-4b60-a5e6-03d14e038766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13) (13, 13) (13, 13)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "graph = AdjMatrixGraph()\n",
    "A, A_binary, A_binary_with_I = graph.A, graph.A_binary, graph.A_binary_with_I\n",
    "f, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(A_binary_with_I, cmap='gray')\n",
    "ax[1].imshow(A_binary, cmap='gray')\n",
    "ax[2].imshow(A, cmap='gray')\n",
    "plt.show()\n",
    "print(A_binary_with_I.shape, A_binary.shape, A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2fd98-4a68-4c31-95c3-147a08b58303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a171cdc3-62bd-4f6f-8fa5-1b83affbecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 30, 52)\n",
      "[[0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(30, 13*4))\n",
    "print(np.shape(inputs))\n",
    "#A_binary = create_adjacency_matrix_tensor(inputs)\n",
    "print((A_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5a81ff-ec1e-4321-9a56-4c5aed1da730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 24, 30, 30)\n",
      "TCN_res (512, 12, 30, 30)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "TCN_res (512, 12, 30, 30)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "===out (512, 12, 30, 5)\n",
      "(512, 12, 30, 30)\n",
      "(512, 12, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "from utils.MS_GCN import MultiScale_GraphConv as MS_GCN\n",
    "from utils.MS_TCN import MultiScale_TemporalConv as MS_TCN\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "ndarray = K.get_value(A_binary)\n",
    "\n",
    "\n",
    "#msgcn = MultiScale_GraphConv(num_scales=15, in_channels=4, out_channels=64, A_binary=ndarray)\n",
    "msgcn = MS_GCN(num_scales=6, in_channels=4, out_channels=30, A_binary=ndarray, disentangled_agg=True)\n",
    "mstcn = MS_TCN(30,30,stride=2)\n",
    "mstcn1 = MS_TCN(30,30,stride=1)\n",
    "dense = tf.keras.layers.Dense(64)\n",
    "\n",
    "# Batch, channels, frames, Nodes (B,C,T,V)\n",
    "# Batch frames, nodes, channels\n",
    "\n",
    "#inputs = tf.keras.layers.Input(shape=(4, 30, 13))\n",
    "inputs = tf.random.normal((512, 4, 30, 13))\n",
    "\n",
    "asd = msgcn(  inputs  )\n",
    "print( np.shape(asd) )\n",
    "asd = mstcn(asd)\n",
    "asd = mstcn1(asd)\n",
    "print( np.shape(asd) )\n",
    "asd = dense(asd)\n",
    "#asd = msgcn(num_gcn_scales, 3, c1, A_binary, disentangled_agg=True)\n",
    "\n",
    "print( np.shape(asd) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a7e1b-874b-4570-b900-4cdad7606906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9bc79-0c60-4b7e-9d79-d382512c3718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a57746-c47c-450c-88b2-26dcb9cc8d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
